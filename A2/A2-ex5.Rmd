---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 5
In our regression analysis, the response variable is "expend" and the explanatory variables are: "bad, crime, lawyers, employ and pop". The purpose is to explain expend by a numerical function of the explanatory variables.

**a)**
<!-- Make some graphical summaries of the data --> First, we make a graphical summary of the data by plotting each variable against the others in scatterplots. Looking at the plots, we observe that expend, lawyers, employ and pop all approximate a linear relationship with each other. Furthermore, state and crime have nonlinear relationhsips with all the other variables. Lastly, the variable bad can be argued to have a weak linear relationship with the variables expend, lawyers, employ and pop. Thereafter, we constructed histograms of the numerical data. Looking at the histograms, it is interesting to see that almost all variables (expend, bad, lawyers, employ and pop) follow a similar pattern, namely, the lowest value appears frequently and as the value increases, the frequency decreases steeply. Except for a few outliers of frequently occuring high values. In contrast, the variable crime shows a different pattern. Namely, the values in the middle occur also relatively frequently. But the rule: as the value increases, the frequency decreases, applies as well. Afterwards, we constructed boxplots of the data. Again, we observe a similar pattern of all variables except the variable crime. Crime is more evenly distributed and contains fewer outliers. The other variables are skewed towards the lower values combined with outlying higher values. To explore these outliers further and in order to build an intuition of the linear relationship between the response variable (expand) and the explanatory variables, we zoomed in on the relevant scatter plots that were presented above. From these plots, we observe a strong linear relationship between the response variable expend and the variables: bad, lawyers, employ and pop. When plotting the simple regression models in these scatterplots, we observe that the outliers follow the linear pattern.
<!-- Scatter plots -->
```{r fig.width=8, fig.height=4, echo=FALSE}
data=read.table(file="expensescrime.txt",header=TRUE)
plot(data)
```
<!-- Histograms -->
```{r fig.width=8, fig.height=2, echo=FALSE}
par(mfrow=c(1,ncol(data)-1)) # minus the state column
for (i in 2:ncol(data)) hist(data[,i],xlab=names(data)[i],main=NULL)
```
<!-- Boxplots -->
```{r fig.width=8, fig.height=3, echo=FALSE}
par(mfrow=c(1,ncol(data)-1)) # minus the state column
for (i in 2:ncol(data)) boxplot(data[,i],main=names(data)[i])
```
<!-- Zoomed scatterplots + abline -->
```{r fig.width=8, fig.height=3, echo=FALSE}
par(mfrow=c(1,5))
plot(data$bad, data$expend,xlab="bad",ylab="expend");abline(lm(expend ~ bad, data=data))
plot(data$crime, data$expend,xlab="crime",ylab="expend");abline(lm(expend ~ crime, data=data))
plot(data$lawyers, data$expend,xlab="lawyers",ylab="expend");abline(lm(expend ~ lawyers, data=data))
plot(data$employ, data$expend,xlab="employ",ylab="expend");abline(lm(expend ~ employ, data=data))
plot(data$pop, data$expend,xlab="pop",ylab="expend");abline(lm(expend ~ pop, data=data))
```
<!-- Investigate the problem of potential and influence points--> A potential point is an outlier in an explanatory variable. The effect can be studied by fitting the model with and without the potential point. If the estimated parameters change drastically when removing the potenetial point, the observation is called an influence point. Using the Cook's formula, the distance of an observation on the predictions can be calculated. Whenever the Cook's distance for an observation approximates or is larger than 1, the observation can be considered to be an influence point. As we have not constructed a model yet, we analyse the potential and influence points of our chosen model in c). <!-- Investigate the problem of collinearity--> Another relevent concept is collinearity. This is the problem of linear relations between explanatory variables. Collinearity can be detected by a straight line in a scatter plot or by calculating the correlation coefficient. Looking at the scatter plots of the data, we suspect collinearity between the variables bad, lawyers, employ and pop. We confirm this by calculating the correlaction coeffiecients of all possible variable combinations. Looking at the output below, we observe that all the combinations of the variables lawyers, employ and pop have a correlation coefficient above 93. Thus, we conclude that these variables have a collinear relation. The variable bad has a weaker collinear relation with the variables lawyers, employ and pop, namely, ranging from 0.83 to 0.93. Lastly, the variable crime has no collinear relation with any of the other variables. When collinearity is detected among variables, we should avoid having both explanatory variables in the model.
```{r echo=FALSE}
round(cor(data[,3:7]),2) # exclude state and expend
```
**b)**
<!-- Fit a linear regression model to the data --> <!-- Use both the step-up and the step-down method to find the best model --> To fit a linear regression model to the data, first, we start with the step-up method. Using this method, we start by fitting all possible simple linear regression models and calculate the determination coefficient ($R^{2}$). The results are shown in the table below (Round 1). Looking at this table, we observe that employ has the largerst value of $R^{2}$ (0.954) and is thus selected. Therefore, we add the remaining variables to contruct a model with the variable employ. The results are shown in the table below (Round 2). We observe that the model that is constructed using the variables employ and lawyers has the highest value of $R^{2}$ (0.9632). This value is also higher than the $R^{2}$ value of the previous model (0.954) and is significant (the p-values are smaller than the significance level of 0.05). For this reason, we extend the model with the remaining variables. The results are shown in the table below (Round 3). The table constructed using the variables employ, lawyers and bad has the highest value of $R^{2}$ (0.9639). However, the result in insignificant (p-value is equal to 0.34496), therefore, the method stops. The resulting model (expend ~ lawyers + employ) is expend = -1.107e$e^{+02}$ + 2.686$e^{-02}$ * lawyers + 2.971$e^{-02}$ * employ + error.

**Round 1:**

| **Explantory Variable(s)** | bad    | crime  | lawyers | employ | pop    |
|----------------------------|--------|--------|---------|--------|--------| 
| **Multiple R-squared**     | 0.6964 | 0.1119 | 0.9373  | 0.954  | 0.9073 |
|----------------------------|--------|--------|---------|--------|--------| 

**Round 2:**

| **Explantory Variable(s)** | employ, bad | employ, crime | employ, lawyers | employ, pop |
| ---------------------------|-------------| --------------| --------------- |-------------|
| **Multiple R-squared**     |0.9551       | 0.9551        | 0.9632          | 0.9543      |
| ---------------------------|-------------| --------------| --------------- |-------------|

**Round 3:**

| **Explantory Variable(s)** | employ, lawyers, bad | employ, lawyers, crime | employ, lawyers, pop |
| ---------------------------|--------------------- | ---------------------- |----------------------|
| **Multiple R-squared**     |0.9639                | 0.9632                 | 0.9637               |

```{r echo=FALSE,results='hide'}
# Round 1
summary(lm(expend~bad,data=data))
summary(lm(expend~crime,data=data))
summary(lm(expend~lawyers,data=data))
summary(lm(expend~employ,data=data))
summary(lm(expend~pop,data=data))
# Round 2
summary(lm(expend~employ+bad,data=data))
summary(lm(expend~employ+crime,data=data))
summary(lm(expend~employ+lawyers,data=data))
summary(lm(expend~employ+pop,data=data))
# Round 3
summary(lm(expend~employ+lawyers+bad,data=data))
summary(lm(expend~employ+lawyers+crime,data=data))
summary(lm(expend~employ+lawyers+pop,data=data))
```
Second, we use the step-down method. This method starts with fitting all explanatory variables in the so-called full model. In each iteration, one explanatory variable is removed. In Round 1, we observe that the variable crime has the highest p-value, 0.25534 > 0.05, therefore, the variable crime will be removed. In Round 2, pop has the highest p-value, 0.06012 > 0.05, therefore, the variable pop will be removed. In Round 3, bad has the highest p-value, 0.34496 > 0.05, therefore, the variable bad will be removed. In Round 4, lawyers has the highest p-value, 0.00113 < 0.05, therefore, the variable will not be removed and the method stops. This results in the model expend = -1.107$e^{+02}$ + 2.686$e^{-02}$ * lawyers + 2.971$e^{-02}$ * employ + error. <!-- If step-up and step-down yield two different models, choose one and motivate your choice --> The step-up and step-down model resulted in the same model. Note that the explanatory variables in this model are linearly correlated, this is bad practise for a regression model. If the step-up and step-down resulted in different models, the one with the higest value of $R^{2}$, the lowest number of explanatory variables and no collinear variables would be prefered. 

**Round 1: expend ~ bad, crime, lawyers, employ, pop**

| **Explantory Variables** | bad | crime | lawyers | employ | pop   | 
|--------------------------|-----|-------|---------|--------| ----  |
| **p-value**              |0.02719|0.25534|0.00592|0.00354 |0.03184|

**Round 2: expend ~ bad, lawyers, employ, pop**

| **Explantory Variables** | bad | lawyers | employ | pop | 
|--------------------------|-----|---------|--------| ----|
| **p-value**              |0.05402|0.00106|0.00380|0.06012|

**Round 3: expend ~ bad, lawyers, employ**

| **Explantory Variables** | bad | lawyers | employ |
|--------------------------|-----|---------|--------|
| **p-value**              |0.34496|0.00147|1.2$e^{-06}$|

**Round 4: expend ~ lawyers, employ**

| **Explantory Variables** | lawyers | employ |
|--------------------------|---------|--------|
| **p-value**              |0.00113|4.89$e^{-07}$|

```{r echo=FALSE,results='hide'}
summary(lm(expend~bad+crime+lawyers+employ+pop,data=data))
summary(lm(expend~bad+lawyers+employ+pop,data=data))
summary(lm(expend~bad+lawyers+employ,data=data))
summary(lm(expend~lawyers+employ, data=data))
```

**c)**
<!-- Check the model assumptions by using relevant diagnostic tools --> We check the model (expend ~ lawyers + employ) assumptions (linearity of the relation and normality of the errors) using both graphical and numerical tools.
<!-- Investigate outliers, potential and influence points --> Coming back to question a), we investigate the outliers of the model. 


If we sort the residuals, we see that the there are outliers on positions 10, 15, 32, 44 and 5. Now we will test whether these points significantly deviate from the other points using the mean shift outlier model. Afterwards, we calculate the Cooks Distance to detect the influence points. Only the datapoint on the fifth position has a Cook Distance that is larger than or close to 1. From the plot of the Cook's distance we can also clearly observe that this value is larger than the others.
<!-- #1 Scatter plot: plot Y against each X k separately -->
First, we look at the scatter plot of the response variable against each explanatory variable separately. This is visualized in a). For each combination with expend, we see two outliers at the right of the scatter plots. Moreover, the relation with the variables bad, lawyers, employ and pop is linear and nonlinear combined with state and crime. 
<!-- #2 Scatter plot of residuals against each Xk in the model separately -->
Second, we construct the scatter plot of the residuals against each explanatory variable that is in the model (crime and employ) seperately. The plot with crime contains a cluster around the middle line and the plot with employ has a cluster in the bottom left diagnal with two outliers in the upper right corner. This means...
<!-- #3 Added variable plot of residuals of Xj against residuals of Y with omitted Xj -->
Third, we construct the added variable plot. In this plot, the residuals of the explanatory variables are plotted against the residuals of the model without that specific variable. This shows the effect of adding an explanatory variable to the model. Looking at the figure, we observe that the plots for bad, lawyers and pop contain compact clusters. The plots of crime and employ are more spread. This means...
<!-- #4 Scatter plot of residuals against each Xk not in the model separately -->
Next, we construct the scatter plots of the residuals against each explanatory variable that is not in the model (bad, lawyers and pop) seperatly. The outputs are similar to each other. When looking at the pattern we do not observe a linear relation and therefore should not include more variables.
<!-- #5 Scatterplot of residuals against Y and Y^ -->
Afterwards, we construct the scatter plot of the reduals against the response variable and the fitted model. We observe little spread as in both plots there is a cluster around zero with only few outliers. This means...
<!-- #6 normal QQ-plot of the residuals and Shapiro-Wilk's method -->
Lastly, we check the normality assumption by constructing a qq-plot and conduction Shapiro-Wilk's test. We cannot assume normality as the qq-plot does not approximate a straight line, this means that the model is invalid. Unfortunetely, none of the other models resulted in a normal distribution of the residuals. When looking at the scatterplots

```{r fig.width=8, fig.height=4, echo=FALSE}
expendlm = lm(expend~crime+employ,data=data)
round(residuals(expendlm),2)
order(abs(residuals(expendlm)))
round(cooks.distance(expendlm),2)
plot(1:51,cooks.distance(expendlm),type="b")
qqnorm(residuals(expendlm))

par(mfrow=c(2,4))
#2 Scatter plot of residuals against each Xk in the model separately.
plot(residuals(expendlm),data[,4],xlab="residuals",ylab="crime")
plot(residuals(expendlm),data[,6],xlab="residuals",ylab="employ")
#4 Scatter plot of residuals against each Xk not in the model separately
plot(residuals(expendlm),data[,3],xlab="residuals",ylab="bad")
plot(residuals(expendlm),data[,5],xlab="residuals",ylab="lawyers")
plot(residuals(expendlm),data[,7],xlab="residuals",ylab="pop")
#5 Scatterplot of residuals against Y and Y^
plot(residuals(expendlm),data[,2],xlab="residuals",ylab="expend")
plot(residuals(expendlm),fitted(expendlm),xlab="residuals",ylab="fitted model")

par(mfrow=c(2,3))
#3 Added variable plot of residuals of Xj against residuals of Y with omitted Xj
x = residuals(lm(bad~employ+crime,data=data))
y = residuals(lm(expend~employ+crime,data=data))
plot(x,y,main="Added variable plot for bad")
x = residuals(lm(crime~bad+lawyers+pop,data=data))
y = residuals(lm(expend~bad+lawyers+pop,data=data))
plot(x,y,main="Added variable plot for crime")
x = residuals(lm(lawyers~employ+crime,data=data))
y = residuals(lm(expend~employ+crime,data=data))
plot(x,y,main="Added variable plot for lawyers")
x = residuals(lm(employ~bad+lawyers+pop,data=data))
y = residuals(lm(expend~bad+lawyers+pop,data=data))
plot(x,y,main="Added variable plot for employ")
x = residuals(lm(pop~employ+crime,data=data))
y = residuals(lm(expend~employ+crime,data=data))
plot(x,y,main="Added variable plot for pop")

#6 normal QQ-plot of the residuals and Shapiro-Wilk's method
qqnorm(residuals(expendlm))
#shapiro.test(residuals(expendlm))
```

#Tests when deleting outliers of the data
```{r}
par(mfrow=c(1,5))
plot(data$bad, data$expend,xlab="expend",ylab="bad", col="red");abline(lm(expend ~ bad, data=data), col="blue")
plot(data$crime, data$expend,xlab="expend",ylab="bad", col="red");abline(lm(expend ~ crime, data=data), col="blue")
plot(data$lawyers, data$expend,xlab="expend",ylab="bad", col="red");abline(lm(expend ~ lawyers, data=data), col="blue")
plot(data$employ, data$expend,xlab="expend",ylab="bad", col="red");abline(lm(expend ~ employ, data=data), col="blue")
plot(data$pop, data$expend,xlab="expend",ylab="bad", col="red");abline(lm(expend ~ pop, data=data), col="blue")

crime = data.frame(data$crime)
order(data$crime)
crime_nooutliers = data.frame(crime[-c(10,8), ])
employ = data.frame(data$employ)
order(data$employ)
employ_nooutliers = data.frame(employ[-c(35,5), ])
#expendlm_nooutliers = lm(expend~crime_nooutliers+employ_nooutliers,data=data)
```

#Tests when deleting influence points in the model
```{R}
expendlm = lm(expend~crime+employ,data=data)
round(residuals(expendlm),2)
order(abs(residuals(expendlm)))
round(cooks.distance(expendlm),2)
plot(1:51,cooks.distance(expendlm),type="b")
qqnorm(residuals(expendlm))
residuals=data.frame(residuals(expendlm))
residuals=residuals[-c(5), ]

```
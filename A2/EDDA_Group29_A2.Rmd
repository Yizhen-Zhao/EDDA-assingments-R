---
title: "**EDDA Group 29 Assignment 2**"
author: "Geoffrey van Driessel (12965065), Yizhen Zhao (2658811) & Sophie Vos (2551583)"
date: "3/9/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=4) 
```

## Exercise 1

**a)** A randomized design with two categorical factors, with 

1. the first factor having three categorical levels and
2. the second factor having two levels and
3. having three samples for each unique categorie

can be produced with the following R code:
``` {r echo=TRUE, eval=FALSE}
I=3; J=2; N=3
rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J)))
```

**b)** The boxplot and interaction plot below confirms our intuition:

1. A cold environment causes a much slower decay
2. Wet bread has a much wider distribution (variance)
3. On average dry bread decays slower than wet bread
4. However, wet and cold (frozen) bread has the slowest decay

From the non-parallel lines in the interaction plot and the wide distribution of the wet sample we conclude that the (wet) humidity amplifies the effect of the temperature and thus it can be explained by the strong interaction between the two factors (opposed to the errors in the measurement).

```{r}
bread = read.table("bread.txt", header = TRUE)

par(mfrow=c(1,2))
boxplot(hours ~ humidity, data = bread)
boxplot(hours ~ environment, data = bread)
```

```{r}
  
interaction.plot(bread$environment, bread$humidity, bread$hours,
        xlab = "Temperature", ylab = "Hours", trace.label = "Humidity")
```

**c)** We have 3 hyphotheses here:
H0:There is no main effect of first factor (humidity)
H0:There is no main effect of second factor (environment)
H0:There is no interactions between two factors
From the two-way anova resulst below, we reject both hyphotheses which means both factors have a main effect on the decay time of bread, and the factors have an interaction effect.
```{r}
breadaov=lm(hours~humidity*environment, data = bread); 
anova(breadaov)
```

**d)**-- need review ---
According to the means of squares, on avergae the environment has the biggest effect on the decay. However this can not be concluded so easily, because it is being compared to one base (the first categorie), instead of a more comprehensive annalysis.

```{r}
summary(breadaov)$coefficients
```

**e)** The first requirements is that for each unique categorie, there should be at least 2 samples, which is the case. Then the most important requirement is that the data among the factors should approximiatly have equal variances. This has been tested in b). and the conclusions was that they approximitally were the same. A different test we can do after the ANOVA test, is check whether the error is normally distributed, which is to be expected of a random variable. In the following QQplot it can be seen that the residuals are approximiatly normally distributed. And in the fitted residuals plot it can be seen that the spread is approxomitelly horizentally symmetric among the fitted values, however there are 2 outliers in the middle. 

```{r}
par(mfrow=c(1,2))
qqnorm(residuals(breadaov))
plot(fitted(breadaov),residuals(breadaov))
```

## Exercise 2

**a)** The following code generates a random block design with five blocks, a factor with three levels, and one sample per unique categorie.

```{r echo=TRUE, eval=FALSE}
B=5;
if1 = sample(1:5)
if2 = sample(6:10)
if3 = sample(11:15)
for (i in 1:B) print(c(if1[i], if2[i], if3[i]))
```

**b)** The boxplots below suggest that indeed the skill level and the interfaces matter for the search time. Where skill level 1 is indeed the fastest, and interface 1 is the fastest from the three interfaces.
And from the interaction plots below it shows clear interaction effects and the overall the factors have the same pattern; all lines start in the lower left corner and end towwards the upper right corner. However, they are not perfectly parallel, this can be explained by the small sample sizes which causes local irregularities. Thus we conclude that there is no interaction between the two factors. 

```{r}
search = read.table("search.txt", header = TRUE)

par(mfrow=c(1,2))
boxplot(time~skill, data=search)
boxplot(time~interface, data=search)

interaction.plot(search$skill,search$interface, search$time,
                 xlab = "Skill level", ylab = "Time", trace.label = "Interface")
interaction.plot(search$interface,search$skill, search$time,
                 xlab = "Interface", ylab = "Time", trace.label = "Skill level")

```

**c)** H0: Search time is the same for all interfaces. From the ANOVA results below it can be concluded that H0 is rejected which means the search time is not the same for all interfaces. Futhermore, we can estimate the time it takes for a user with skill level 3 to find a product using interface 2 by looking at the summary table and adding the coeffcients of these two categories to the incercept. Thus that would be 15.015+3.033+2.7=20.748.

```{r}
search$skill = factor(search$skill)
search$interface = factor(search$interface)

aovsearch=lm(time~interface+skill, data=search)
anova(aovsearch)
summary(aovsearch)
```

**d)** The QQ-plot of the residuals below looks normally distributed, which is good. The fitted residuals do not depict any outliers. 

```{r}
par(mfrow=c(1,2))
qqnorm(residuals(aovsearch))
plot(fitted(aovsearch),residuals(aovsearch))
```

**e)** The result of the Friedman test is the same as the ANOVA: we reject the H0 mentioned before, thus there is a difference in sesarch times.

```{r}
friedman.test(search$time,search$interface,search$skill)
```

**f)** The one-way ANOVA returns no significant difference in the search time between the interfaces. This result is not very usefull, because 1) we removed a lot of information from the model and 2) the model now assumes that the block is a random selection of all available blocks, which is not the case because the blocks were fixed/predetermined.

```{r}

aovsearch2=lm(time~interface, data=search)
anova(aovsearch2)

```

## Exercise 3
**a)** First we evaluate normality to determine which test to use. From the QQ-plots below we conclude that both treatment samples are normally distributed. 

```{r}
cow = read.table("cow.txt", header = TRUE);

#There are two factors influence the milk production
mm = subset(cow, treatment=="A")["milk"]
sf = subset(cow, treatment=="B")["milk"]
mm1 = as.numeric(unlist(mm))
sf1 = as.numeric(unlist(sf))

par(mfrow=c(1,2))
qqnorm(mm1)
qqnorm(sf1)
```

There are three hypothese:
H0 (id): There is no difference in milk production within cows
H0 (per): There is no difference in milk production with different period 
H0 (treatment): There is no difference in milk production with different treatment

From the ANOVA results below, we can conclude that within-cow variation(see variable "id") the milk production differs. Because the p-value for id is less than 0.05, therefore, the first H0 is rejected. Furthermore, from the summary we can conclude that most of the cows(except id4) are different from the cow with id1.  
Then, we could see p-value for per is less than 0.05 so we reject the second H0 which means whether a cow is going through the first period or second seems to make a difference.
Furthurmore, because the p-value of treatment is 0.51654, so we do not reject the third H0 which means the treatment A does not significant differ from treatment B. Also this could be seen from second table (treatmentB). 
Also the summary below indicate that there is no significant difference in milk production. However, it is important to note that this is not the appropiate way of testing the cross-over design.


```{r}
cow$id = factor(cow$id); cow$per = factor(cow$per)
cowanova = lm(milk~id+per+treatment,data = cow)
anova(cowanova)
summary(cowanova)
``` 

**b)** modelling the cow effect as a random effect (use the function lmer)
```{r}
library(lme4)
cow$order=factor(cow$order)
cowlmer = lmer(milk~treatment+order+per+(1|id), data=cow, REML = FALSE)
summary(cowlmer)
```

Based on the p-value below, we do not reject H0 for treatment we mentioned before which means treatment is not important. The result is the same as that in a).
```{r}
cowlmerTreatment = lmer(milk~order+per+(1|id), data=cow, REML = FALSE)
anova(cowlmerTreatment, cowlmer)
```

Based on the p-value below, we do not reject H0: There is no difference in milk production with different order. Therefore, the order of treatment AB is not important. 
```{r}
cowlmerOrder = lmer(milk~treatment+per+(1|id), data=cow, REML = FALSE)
anova(cowlmerOrder, cowlmer)
```

Based on the p-value below, we reject H0 for per we mentioned before which means whether a cow is going through the first treatment or second is important. 
```{r}
cowlmerPer = lmer(milk~treatment+order+(1|id), data=cow, REML = FALSE)
anova(cowlmerPer, cowlmer)
```

**a)** From this result below that p-value is 0.8281 and we do not reject H0 for treatment which means the treatment is not important. From previous analysis the same conclusion was reached. Given the design, it is inappropriate to use the paired t-test. Since the previous analysis shows factors such as per has significant effect on milk production, it might be unwise to ignore such factor.
```{r}
attach(cow)
t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
```

## Exercise 4

**a)**
```{r}
nausea = read.table("nauseatable.txt", header = TRUE)
nausea
```

```{r}
#We create a datafeam contains 2 columns: nausea, medicine
#Set patient suffered from nausea as 1, otherwise 0
nausea=c(rep(0,times=100), rep(1,times=52),rep(0, times=32), rep(1, times=35),rep(0, times=48), rep(1, times=37))

medicine=c(rep("Chlorpromazine", times=152),rep("Pentobarbital(100mg)", times=67),rep("Pentobarbital(150mg)", times=85))
newNausea = data.frame(nausea,medicine); newNausea
```

```{r}
xtabs(~medicine+nausea, data = newNausea) 
```
We could see that the result is the same as the dataset "nauseatable.txt", so the dataframe we created is correct. 

**b)**
```{r}
#Permutation test
label_medic = factor(newNausea$medicine)
nausdata = data.frame(nausea=as.vector(newNausea$nausea), label_medic)

attach(nausdata)
mystat=function(x) sum(residuals(x)^2)
B=1000
tstar=numeric(B)
for(i in 1:B){
  treatstar = sample(label_medic)
  tstar[i] = mystat(lm(nausea~treatstar))
}
myt = mystat(lm(newNausea$nausea~newNausea$medicine))
hist(tstar)
myt
pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
pl
```
We could see that p-value is 0.041, less than 0.05. Therefore, H0 should be rejected which means the medicine doesn't work equally well against nausea. 

```{r}
#chisquare test
chisq.test(xtabs(~newNausea$medicine+newNausea$nausea))[[1]]
```
By using chisquared test we could get the result which is 6.62 and we could use this to calculate the p-value. While in permutation test the result is 71.82, they use different values to calculate the final p-value.

**c)**
```{r}
chisq.test(xtabs(~newNausea$medicine+newNausea$nausea))
```
We could see from chisquared test that p-value is 0.03643, smaller than 0.05, therefore, H0 should be rejected. And the p-value of both tests are close to each other. In conclusion, they use different methods but the results are the same. 

## Exercise 5
In our regression analysis, the response variable is "expend" and the explanatory variables are: "bad, crime, lawyers, employ and pop". The purpose is to explain expend by a numerical function of the explanatory variables.

**a)**
<!-- Make some graphical summaries of the data -->
First, we make a graphical summary of the data by plotting each variable against the others. Furthermore, we construct a histogram of all the numerical variables. Looking at the plots, we observe that expend, lawyers, employ and pop all approximate a linear relationship with each other. Furthermore, state and crime have nonlinear relationhsips with all the other variables. Lastly, the variable bad can be argued to have a weak linear relationship with the variables expend, lawyers, employ and pop. Looking at the histograms, it is interesting to see that almost all variables (expend, bad, lawyers, employ and pop) follow a similar pattern, namely, the lowest value appears frequently and as the value increases, the frequency decreases steeply. Except for a few outliers of frequently occuring high values. In contrast, the variable crime shows a different pattern. Namely, the values in the middle occur also relatively frequently. But the rule: as the value increases, the frequency decreases, applies as well.
<!-- Scatter plots -->
```{r fig.width=8, fig.height=4, echo=FALSE}
data=read.table(file="expensescrime.txt",header=TRUE)
plot(data)
```
<!-- Histograms -->
```{r fig.width=8, fig.height=2, echo=FALSE}
par(mfrow=c(1,ncol(data)-1)) # minus the state column
for (i in 2:ncol(data)) hist(data[,i],main=names(data)[i])
```

To build an intuition of the linear relationship between the depandable variable (expand) and its explanatory variable, we have plotted the explanatory variables against the dependaple variable below. These are the same plots as from the first plot with all factors plotted against each other, but now we can take a closer look. From the plots we can see a strong linear relationship between the dependable expend and 4 of the five factors: bad, lawyers, employ and pop. However we see very big outliers, which will have to be adjusted for, otherwise they will have a very big weight in the factor coefficient from the linear regression.
```{r fig.width=8, fig.height=4, echo=FALSE}

par(mfrow=c(1,3))
plot(data$expend, data$bad)
plot(data$expend, data$crime)
plot(data$expend, data$lawyers)
plot(data$expend, data$employ)
plot(data$expend, data$pop)
```
<!-- Investigate the problem of potential and influence points-->
A potential point is an outlier in an explanatory variable. The effect can be studied by fitting the model with and without the potential point. If the estimated parameters change drastically when removing the potenetial point, the observation is called an influence point. Using the Cook's formula, the distance of an observation on the predictions can be calculated. Whenever the Cook's distance for an observation approximates or is larger than 1, the observation can be considered to be an influence point. As we have not constructed a model yet, we analyse the potential and influence points of our chosen model in c).
<!-- Investigate the problem of collinearity-->
Another relevent concept is collinearity. This is the problem of linear relations between explanatory variables. Collinearity can be detected by a straight line in a scatter plot or by calculating the correlation coefficient. Looking at the scatter plots of the data, we suspect collinearity between the variables expend, lawyers, employ and pop. We confirm this by calculating the correlaction coeffiecients of all possible variable combinations. Looking at the output below, we observe that all the combinations of the variables expend, lawyers, employ and pop have a correlation coefficient above 93. Thus, we conclude that these variables have a collinear relation. The variable bad has a weaker collinear relation with the variables expend, lawyers, employ and pop, namely, ranging from 0.83 to 0.93. Lastly, the variable crime has no collinear relation with any of the other variables. When collinearity is detected among variables, we should avoid having both explanatory variables in the model.
```{r echo=FALSE}
round(cor(data[,2:7]),2)
```
**b)**
<!-- Fit a linear regression model to the data -->
<!-- Use both the step-up and the step-down method to find the best model -->
To fit a linear regression model to the data, first, we start with the step-up method. Using this method, we start by fitting all possible simple linear regression models and calculate the determination coefficient ($R^{2}$). The results are shown in the table below. Looking at this table, we observe that employ has the largerst value of $R^{2}$ (0.954) and is thus selected. Next, we combine this variable with all the variables that do not have a collinear relation with employ. These are the explanatory variables bad and crime. Note that the variable bad still can be considered to be linearly correlated to employ. Adding the variables bad and crime to the models yields in $R^{2}$ = 0.9551. This is an improvement compared to the previous model. Therefore, we continue to add the other vairables to the model. For both models, there is just one variable to add. This results in the last possible option: a model of employ, bad and crime combined. This result in $R^{2}$ = 0.9568, the highest value so far. As there are no more variables to add, the method stops here. The resulting model is: expend = -2.857$e^{+02}$ + 4.979$e^{-02}$ * employ - 1.391$e^{+00}$ * bad + 3.810$e^{-02}$ * crime + error. We have to be carefull as it could be argued that the variables employ and bad are collinear. Therefore, the model that is constructed with the variables employ and crime (expend = -2.484$e^{+02}$ + 4.630$e^{-02}$ * employ + 2.962$e^{-02}$ * crime + error) might be a better model as it contains fewer variables and the value of $R^{2}$ is similar.

| **Explantory Variable(s)** | bad    | crime  | lawyers | employ | pop    | bad, employ | crime, employ | bad, crime, employ |
|---------------------|--------|--------|---------|--------|--------| -------------|--------------- | --------------- |
| **Multiple R-squared**  | 0.6964 | 0.1119 | 0.9373  | 0.954  | 0.9073 | 0.9551       | 0.9551         | 0.9568  |

```{r echo=FALSE,results='hide'}
summary(lm(expend~bad,data=data))
summary(lm(expend~crime,data=data))
summary(lm(expend~lawyers,data=data))
summary(lm(expend~employ,data=data))
summary(lm(expend~pop,data=data))
summary(lm(expend~bad+employ,data=data))
summary(lm(expend~crime+employ,data=data))
summary(lm(expend~bad+crime+employ,data=data))
```
Second, we use the step-down method. This method start with fitting all explanatory variables in the so-called full model. In each iteration, one explanatory variable is removed. This time, we try the model with all variables, regardless of collinearity. In round 1, we observe that the variable crime has the highest p-value, 0.25534 > 0.05, therefore, the variable crime will be removed. In round 2, pop has the highest p-value, 0.06012 > 0.05, therefore, the variable pop will be removed. In round 3, bad has the highest p-value, 0.34496 > 0.05, therefore, the variable bad will be removed. In round 4, lawyers has the highest p-value, 0.00113 < 0.05, therefore, the variable will not be removed and the method stops. This results in the model expend = -1.107$e^{+02}$ + 2.686$e^{-02}$ * lawyers + 2.971$e^{-02}$ * employ + error.

**Round 1: expend ~ bad, crime, lawyers, employ, pop**

| **Explantory Variables** | bad | crime | lawyers | employ | pop | 
|--------------------------|-----|-------|---------|--------| ----|
| **p-value**              |0.02719|0.25534|0.00592|0.00354|0.03184|

**Round 2: expend ~ bad, lawyers, employ, pop**

| **Explantory Variables** | bad | lawyers | employ | pop | 
|--------------------------|-----|---------|--------| ----|
| **p-value**              |0.05402|0.00106|0.00380|0.06012|

**Round 3: expend ~ bad, lawyers, employ**

| **Explantory Variables** | bad | lawyers | employ |
|--------------------------|-----|---------|--------|
| **p-value**              |0.34496|0.00147|1.2$e^{-06}$|

**Round 4: expend ~ lawyers, employ**

| **Explantory Variables** | lawyers | employ |
|--------------------------|---------|--------|
| **p-value**              |0.00113|4.89$e^{-07}$|

```{r echo=FALSE,results='hide'}
summary(lm(expend~bad+crime+lawyers+employ+pop,data=data))
summary(lm(expend~bad+lawyers+employ+pop,data=data))
summary(lm(expend~bad+lawyers+employ,data=data))
summary(lm(expend~lawyers+employ, data=data))
```
<!-- If step-up and step-down yield two different models, choose one and motivate your choice -->
Using the step-up and step-down method resulted in two different models. The advantage of the model constructed using the step-up method (expend ~ employ bad crime) is that the variables are not collinear. The advantages of the model constructed using the step-down method (expend ~ lawyers employ) are that the value of $R^{2}$ (0.9632) is higher compared to the $R^{2}$ value of the step-up model (0.9568) and the model contains fewer explanatory variables. However, as the collinearity of variables weight higher compared to number of variables and the difference of $R^{2}$ is relatively small, we prefer the model constructed using the step-up model. We even consider removing the variable bad of the step-up model as it can be argued to have a collinear relation with the variable employ.

**c)**
<!-- Check the model assumptions by using relevant diagnostic tools -->
We check the model (expend ~ employ crime) assumptions (linearity of the relation and normality of the errors) using both graphical and numerical tools.
<!-- #1 Scatter plot: plot Y against each X k separately -->
First, we look at the scatter plot of the response variable against each explanatory variable separately. This is visualized in a). For each combination with expend, we see two outliers at the right of the scatter plots. Moreover, the relation with the variables bad, lawyers, employ and pop is linear and nonlinear combined with state and crime. 
<!-- #2 Scatter plot of residuals against each Xk in the model separately -->
Second, we construct the scatter plot of the residuals against each explanatory variable that is in the model (crime and employ) seperately. The plot with crime contains a cluster around the middle line and the plot with employ has a cluster in the bottom left diagnal with two outliers in the upper right corner. This means...
<!-- #3 Added variable plot of residuals of Xj against residuals of Y with omitted Xj -->
Third, we construct the added variable plot. In this plot, the residuals of the explanatory variables are plotted against the residuals of the model without that specific variable. This shows the effect of adding an explanatory variable to the model. Looking at the figure, we observe that the plots for bad, lawyers and pop contain compact clusters. The plots of crime and employ are more spread. This means...
<!-- #4 Scatter plot of residuals against each Xk not in the model separately -->
Next, we construct the scatter plots of the residuals against each explanatory variable that is not in the model (bad, lawyers and pop) seperatly. The outputs are similar to each other. When looking at the pattern we do not observe a linear relation and therefore should not include more variables.
<!-- #5 Scatterplot of residuals against Y and Y^ -->
Afterwards, we construct the scatter plot of the reduals against the response variable and the fitted model. We observe little spread as in both plots there is a cluster around zero with only few outliers. This means...
<!-- #6 normal QQ-plot of the residuals and Shapiro-Wilk's method -->
Lastly, we check the normality assumption by constructing a qq-plot and conduction Shapiro-Wilk's test. We cannot assume normality as the qq-plot does not approximate a straight line, this means that the model is invalid. Unfortunetely, none of the other models resulted in a normal distribution of the residuals. 

```{r fig.width=8, fig.height=4, echo=FALSE}
expendlm = lm(expend~crime+employ,data=data)
par(mfrow=c(2,4))
#2 Scatter plot of residuals against each Xk in the model separately.
plot(residuals(expendlm),data[,4],xlab="residuals",ylab="crime")
plot(residuals(expendlm),data[,6],xlab="residuals",ylab="employ")
#4 Scatter plot of residuals against each Xk not in the model separately
plot(residuals(expendlm),data[,3],xlab="residuals",ylab="bad")
plot(residuals(expendlm),data[,5],xlab="residuals",ylab="lawyers")
plot(residuals(expendlm),data[,7],xlab="residuals",ylab="pop")
#5 Scatterplot of residuals against Y and Y^
plot(residuals(expendlm),data[,2],xlab="residuals",ylab="expend")
plot(residuals(expendlm),fitted(expendlm),xlab="residuals",ylab="fitted model")

par(mfrow=c(2,3))
#3 Added variable plot of residuals of Xj against residuals of Y with omitted Xj
x = residuals(lm(bad~employ+crime,data=data))
y = residuals(lm(expend~employ+crime,data=data))
plot(x,y,main="Added variable plot for bad")
x = residuals(lm(crime~bad+lawyers+pop,data=data))
y = residuals(lm(expend~bad+lawyers+pop,data=data))
plot(x,y,main="Added variable plot for crime")
x = residuals(lm(lawyers~employ+crime,data=data))
y = residuals(lm(expend~employ+crime,data=data))
plot(x,y,main="Added variable plot for lawyers")
x = residuals(lm(employ~bad+lawyers+pop,data=data))
y = residuals(lm(expend~bad+lawyers+pop,data=data))
plot(x,y,main="Added variable plot for employ")
x = residuals(lm(pop~employ+crime,data=data))
y = residuals(lm(expend~employ+crime,data=data))
plot(x,y,main="Added variable plot for pop")

#6 normal QQ-plot of the residuals and Shapiro-Wilk's method
qqnorm(residuals(expendlm))
#shapiro.test(residuals(expendlm))
```
